{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/PU.png\" width=\"100%\">\n",
    "\n",
    "## Course Name : Machine Learning for Professionals (ML 701)    \n",
    "#### Notebook compiled by : Bhushan Garware, Project Lead at Learning and Development  \n",
    "** Important ! ** For internal circulation olny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Naive Bayes Algorithm \n",
    "<img src=\"Images/NB_Intro.jpg\" width=\"50%\">\n",
    "\n",
    "The Naive Bayes algorithm is simple and effective and should be one of the first methods you try on a classification problem. In this tutorial you are going to learn about the Naive Bayes algorithm including how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Naive Bayes algorithm?\n",
    "\n",
    "- Based on Baye's Theorem\n",
    "- Assumes predictors are independent i.e. 'Naive'\n",
    "\n",
    "Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.\n",
    "\n",
    "<img src=\"Images/NB_f.jpg\" width=\"70%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's work on Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the iris datasets\n",
    "dataset = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "model.fit(dataset.data, dataset.target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Make Predictions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expected = dataset.target\n",
    "predicted = model.predict(dataset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Summarize the fit of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variants of Naive Bayes in Scikit Learn\n",
    "\n",
    "Again, scikit learn (python library) will help here to build a Naive Bayes model in Python. There are three types of Naive Bayes model under scikit learn library:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Gausiaan**: It is used in classification and it assumes that features follow a normal distribution.\n",
    "\n",
    "- **Multinomial**:It is used for discrete counts. For example, let’s say,  we have a text classification problem. We have “count how often word occurs in the document”, you can think of it as “number of times outcome number x_i is observed over the n trials”.\n",
    "\n",
    "- **Bernoulli**: The binomial model is useful if your feature vectors are binary (i.e. zeros and ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(dataset.data, dataset.target)\n",
    "expected = dataset.target\n",
    "predicted = model.predict(dataset.data)\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "model = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(dataset.data, dataset.target)\n",
    "expected = dataset.target\n",
    "predicted = model.predict(dataset.data)\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning Parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Naive_bayes(Model_Type):\n",
    "        # import some data to play with\n",
    "        iris = datasets.load_iris()\n",
    "        X = iris.data[:, :2]  # we only take the first two features.\n",
    "        Y = iris.target\n",
    "        h = .02  # step size in the mesh\n",
    "        # we create an instance of Neighbours Classifier and fit the data.\n",
    "        if(Model_Type=='Gaussian'):\n",
    "            model =  GaussianNB()\n",
    "        elif (Model_Type=='Multinomial'):\n",
    "                model =  MultinomialNB()                         \n",
    "        else:\n",
    "                model =  BernoulliNB()  \n",
    "                \n",
    "        model.fit(X, Y)\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "        x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "        y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        \n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.figure(1, figsize=(4, 3))\n",
    "        plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "        # Plot also the training points\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "        plt.xlabel('Sepal length')\n",
    "        plt.ylabel('Sepal width')\n",
    "        plt.xlim(xx.min(), xx.max())\n",
    "        plt.ylim(yy.min(), yy.max())\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.show()\n",
    "        \n",
    "        model.fit(dataset.data, dataset.target)\n",
    "        expected = dataset.target\n",
    "        predicted = model.predict(dataset.data)\n",
    "        print(metrics.classification_report(expected, predicted))\n",
    "        print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.html import widgets\n",
    "from IPython.html.widgets import interact\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = interact(Naive_bayes, Model_Type=('Gaussian','Multinomial','Bernoulli'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {
    "71ba067908d14ef2817dfef0213eb624": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
